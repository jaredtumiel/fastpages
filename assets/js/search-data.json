{
  
    
        "post0": {
            "title": "Friston's Free Energy Principle Explained",
            "content": "The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Fristonâ€™s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we wonâ€™t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundworkâ€¦ . Covered in this post: . Bayesian inference | Phenotype as set of viable states | Entropy and expected surprise | Kullback-Liebler Divergence | Recognition- and Generative-densities | Derivation of â€˜free energyâ€™ term | . Introduction . Karl Fristonâ€™s Free Energy Principle has fascinated and baffled me since I first heard about it in a SlateStarCodex blog post2. Ever since, Iâ€™ve spent a good chunk of my spare time trying to understand the ideas and context that underpin the theory. Fristonâ€™s work is notoriously difficult to understand, something which Friston himself (and definitely the people who read his work) acknowledge with a wry shrug3. This comes down to a few things: . the maths itself is fairly daunting (although - I hope to show - not impenetrable, and well worth understanding), and Fristonâ€™s notation can be opaque until you get used to it. | the fields that Friston draws on to derive the FEP are diverse and any given person is unlikely to have studied them all4. I happen to think this is whatâ€™s most exciting about the FEP, because it provides an excuse to learn more about: Statistical Mechanics, especially non-equilibrium stat-mech | Reinforcement Learning | Neuroscience and Predictive Coding | Dynamic systems and (stochastic) differential equations | Information theory | Variational methods | Path-integral formulations and the Principle of Least Action | Embodied cognition | Bayesian inference, action under uncertainty | Clinical psychiatry and computational corelates of pyschopathology | . | . To begin, letâ€™s look at some motivating ideas, which weâ€™ll keep coming back to, with more and more formalism to back them up. . You: a thing in the world . You are a thing5 and you live in a world which you do not fully understand (or at-least, it should feel that way, if youâ€™re paying any attention). . You get a constant stream of new information through your senses, and your brain somehow needs to use this information-deluge to do things like make exceptional scrambled eggs, renew your driverâ€™s licence, floss your teeth, and other crucial survival-skills. . Our first big insight into the FEP is that if you want to keep doing the survival thing, you should care not about the sense data itself, but about the stuff out there that causes it. The sense data is just receptors firing more or less often. What matters is that you have receptors that reliably fire in a pattern that tells you useful things like: . thereâ€™s a tiger there and she looks pissed | it smells like chocolate cookies have just come out the oven, so you might get fed soon | the molten chocolate cookie you just inhaled is scalding your oesophagus and causing long-term scarring | . So your sense-data isnâ€™t random - it has external causes - but itâ€™s not perfectly reliable either. Itâ€™s a noisy connection, and ambiguity abounds: . is that a burglar standing in the corner of your dark room, or did someone leave a coat hanging on your hatstand? | is that the rustling of the wind in the leaves, or another, even scarier tiger stalking you? | is the pressure of your â€˜deep-tissue sports-massageâ€™ a welcome relief from stiffness, or categorical proof that your masseuse is a sadist? | . In the time you read all of the above, you breathed several times, each of your cells used some ATP, some cells died or were phagocytosed, the state of your brain changed and a bunch of different neurons fired, and yet You are still a thing in the world. From this I infer that you did not suddenly dissolve into an unremarkable puddle of goo in the preceding 30 seconds. This is our second big insight into the FEP: as a system, many little things can change (you are dynamic), but you must keep yourself tightly bound into a larger pattern. Friston often refers to this as possessing an â€˜attracting setâ€™ - a set of states that all of your bizarre chemical processes can wiggle around in and between, but not out of. . There are lots of ways you could configure all of the atoms that currently constitute â€˜youâ€™. Unfortunately, the vast majority are unworkable, probably because the most likely arrangement of those atoms is spread in a thin mist somewhere between here and Neptune. The small subset of all possible ways to configure yourself that keeps you â€˜youâ€™ is your phenotype, and homeostasis is basically the process of keeping yourself in those states through feedback and self-regulation. To survive long-term, you need to have a high probability of occupying those states that are compatible with life (and a low probability of becoming a thin mist somewhere between here and Neptune) . Letâ€™s make a concrete example out of temperature . Itâ€™s getting hot in here (and thatâ€™s surprising) . Your body keeps itself near 37 degrees (Celsius), all the time6 . If you were suddenly to sense a temperature of 800 degrees, that would be surprising. And bad. As an organism, 800 degrees is not compatible with your continued existence, and is not something your phenotype is bequipped to deal with. In this sense, your body is implicitly making a claim that there is a low probability of you experiencing 800 degree heat, because if that were not the case, you would have a different body. Since 37 degrees seems cosy enough, your body itself is an implicit expectation of a high probability of sensing that temperature7. . So your evolved biology â€˜expectsâ€™ 37 degrees , and when it doesnâ€™t get it itâ€™s surprised. This â€˜surprisalâ€™ is actually a formal term from information theory, where we quantify the amount of surprise as the negative natural logarithm of the probability of the observed outcome: . lnâ¡P(X=x) ln P(X = x)lnP(X=x) . All this is saying is that the lower the probability we expect for an event, the more surprised we should be if we do in-fact observe that event, and conversely, if we think there is a high probability of an event happening, we should not be very surprised to see it occur. If you enter the lottery, and I tell you youâ€™ve won, youâ€™d be really really surprised, because you know the probability of that is small. If I tell you youâ€™ve lost, youâ€™re not really surprised at all, as that was always the likely outcome. . . Letâ€™s drill a little deeper into this business of sensing something and - on the basis of that sensation - forming accurate beliefs about the temperature of the environment and your body. Itâ€™s worth saying: you donâ€™t have a nice digital-thermometer organ attached somewhere to your body which your brain can just look at. You have millions of tiny sensory receptors, which fire because of the energetic bumping and jostling of atoms hitting the receptor. For a temperature receptor, when itâ€™s hotter, the atoms hitting it have a higher average energy, which makes it more likely that the neuron the receptor is attached to is depolarised and fires an action potential (â€œspikesâ€). We can roughly reason that in hotter environments, our temperature receptors are firing more often (but only on average - itâ€™s still a â€œnoisyâ€ signal), and so maybe if our brain counted the number of spikes in a certain time, it could learn a mapping from the state of the sensory data it receives to the probable temperature of the environment. . This gives us our third big clue about the FEP: we donâ€™t directly experience the environment, only the noisy sensations that correlate with it. This is really the jumping off point for the FEP: as an organism, we need to minimise our surprisal (we donâ€™t want to find ourselves in 800 degree heat, and do want to find ourselves at 37 degrees, with high probability), but we only have access to our noisy sense data, and we donâ€™t know what causally determines our temperature in the environment. . Getting Bayesian . As organisms, we want to update our beliefs about the true state of the environment, given some sense data as evidence. Thatâ€™s right, itâ€™s time for8 . . (obligatory link to Yudkowsky on Bayes Theorem. If this is new, read this now) . Bayes theorem tells us the optimal way to update our beliefs given some new evidence. Doing this is sometimes called Bayesian inference, or Bayesian updating, or - controversially - as â€œnot making up your beliefsâ€. . . What we want is to update our belief about the state of the environmental temperature, given our current sensory data: . LetÂ P(Temp=37)â‰¡P(T)Let P(Temp = 37) equiv P(T)LetÂ P(Temp=37)â‰¡P(T) . LetÂ P(Sensation=40Â spikesÂ )â‰¡P(S)Let P(Sensation = 40 spikes ) equiv P(S)LetÂ P(Sensation=40Â spikesÂ )â‰¡P(S) . P(T=37âˆ£S=40)P(T=37|S=40)P(T=37âˆ£S=40) . So our hypothesis is that the temperature is 37 degrees, and the evidence weâ€™re using to evaluate it is the 40 spikes we received in the last few seconds. . Now if we assume that our environmental states are a continuous variable (temperature is just a real number) then we can make the denominator into an integral over the environmental states (possible temperatures): . P(T|S)=P(S|T)P(T)âˆ«P(S|T)P(T)dTP left( T middle| S right) = frac{P left( S middle| T right)P left( T right)}{ int_{}^{}{P left( S middle| T right)P left( T right) text{dT}}}P(Tâˆ£S)=âˆ«â€‹P(Sâˆ£T)P(T)dTP(Sâˆ£T)P(T)â€‹ . And there you have it. All you need to do to perfectly understand the world is know the probability of experiencing some sensory data given a particular environmental state, and evaluate the probability of experiencing that sensory data under every possible hypothetical temperature. Very simple stuff! . Okay, so that integral is intractable, which is formal mathematical language for â€˜Wolfram Alpha canâ€™t compute itâ€™. This is a problem for the FEP, and much of the rest of our effort will go into getting around that integral. When we canâ€™t get an exact result, we turn to approximate methods. Weâ€™re going to manipulate our equation above until itâ€™s in a form that allows us to do approximate Bayesian inference. . Approximating your posterior . We want to find the posterior probability (the probability we get after applying a Bayesian update) that: . P(T=37âˆ£S)P(T=37|S)P(T=37âˆ£S) . It would really help us control our body temperature if we understood the causal influences determining it. Intuitively, we want a world-model that predicts that if we move closer to a heat-source, weâ€™ll get sensory data that tells us weâ€™re getting hotter. . The internal model that encodes how we expect our sense data to correlate with our environmental states is called a Generative Model, and is referred to by Friston as the G-density. Our G-density tells us the joint probability of experiencing some sense data and a corresponding environmental state: P(T,S)P(T,S)P(T,S) . You can imagine this as a big table that assigns a probability to each possible pair of values. A good model in this case would assign a high probability to the combination: . P(highÂ temp,lotsÂ ofÂ spikes)P(high temp, lots of spikes)P(highÂ temp,lotsÂ ofÂ spikes) . and a low probability to things like: . P(highÂ temp,fewÂ spikes)P(high temp, few spikes)P(highÂ temp,fewÂ spikes) . We will also want a function that represents our current â€˜best-guessâ€™ as to the causes of our sensory input, which weâ€™ll call the R-density (for Recognition): q(T)q(T)q(T) . This is just a probability distribution over the state of the environment, and weâ€™re using $q$ instead of $p$ to remind us that itâ€™s a distribution weâ€™re guessing at. . We now need another fancy piece of information theory: the Kullback-Liebler (KL) Divergence9 . The KL Divergence just tells us how different two different probability distributions are: . . We want the KL divergence because we want to know how close or far away our best guess is to the true posterior belief if we could compute that ugly integral . We write the KL-divergence between our guess $q(T)$ about the environment and the optimal posterior belief about the environment from Bayeâ€™s theorem. . DKL(q(T)âˆ£âˆ£P(Tâˆ£S))Â =Â âˆ«dTÂ q(T)lnâ¡q(T)P(T|S)D_{ text{KL}}(q(T)||P(T|S)) = int_{}^{}{ text{dT} q( T ) ln frac{q left( T right)}{P left( T middle| S right)}}DKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))Â =Â âˆ«â€‹dTÂ q(T)lnP(Tâˆ£S)q(T)â€‹ . which we rearrange using the property of the logarithm to: . =Â âˆ«dTÂ [q(T)lnâ¡q(T)Â âˆ’Â P(Tâˆ£S)]= int_{}^{}{dT lbrack q left(T right) ln{q(T) - P(T|S) rbrack}}=Â âˆ«â€‹dTÂ [q(T)lnq(T)Â âˆ’Â P(Tâˆ£S)] . Now, any R-density (that is, any $q(T)$) which minimises this KL divergence, must be a good approximation of our true posterior. The only problem is that we donâ€™t know the true posterior (thatâ€™s the thing weâ€™re trying to work out in the first place), and so canâ€™t simply guess a $q(T)$ to see if it minimises the KL Divergence, because we donâ€™t know what weâ€™re comparing it to. . To get around this, weâ€™ll rewrite the true posterior . P(Tâˆ£S)P(T|S)P(Tâˆ£S) . using some pretty basic probability theory identities: . P(T|S)=P(S|T)P(T)P(S)P left( T middle| S right) = frac{P left( S middle| T right)P left( T right)}{P left( S right)}P(Tâˆ£S)=P(S)P(Sâˆ£T)P(T)â€‹ . P(T,S)=P(S|T)P(T)P left( T,S right) = P left( S middle| T right)P left( T right)P(T,S)=P(Sâˆ£T)P(T) . âˆ´P(T|S)=P(T,S)P(S) therefore P left( T middle| S right) = frac{P left( T,S right)}{P left( S right)}âˆ´P(Tâˆ£S)=P(S)P(T,S)â€‹ . If we take the natural log of both sides: . lnâ¡P(T|S)=lnâ¡P(T,S)P(S)=lnâ¡P(T,S)âˆ’lnâ¡P(S) ln{P left( T middle| S right)} = ln{ frac{P left( T,S right)}{P left( S right)} = ln{P left( T,S right)} - ln{P left( S right)}}lnP(Tâˆ£S)=lnP(S)P(T,S)â€‹=lnP(T,S)âˆ’lnP(S) . Plugging this expression for $P(T|S)$ into our KL Divergence we get: . DKL(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«dTÂ [q(T)Â lnâ¡q(T)âˆ’lnâ¡P(T,S)+lnâ¡P(S)]D_{ text{KL}}(q(T)||P(T|S)) = int_{}^{}{dT lbrack q(T) ln{q left( T right) - ln{P left( T,S right) + ln{P left( S right)}}}} rbrackDKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«â€‹dTÂ [q(T)Â lnq(T)âˆ’lnP(T,S)+lnP(S)] . Combining the first two $ ln$ terms: . DKL(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«dTÂ [q(T)Â lnâ¡q(T)P(T,S)Â +Â lnâ¡P(S)Â ]D_{ text{KL}}(q(T)||P(T|S)) = int_{}^{}{dT lbrack q(T) ln{ frac{q left( T right)}{P left( T,S right)} + ln{P left( S right)} }} rbrackDKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«â€‹dTÂ [q(T)Â lnP(T,S)q(T)â€‹Â +Â lnP(S)Â ] . Something you may notice about this is that we now have our KL divergence in terms of only our R-density, $q(T)$, and our G-density, $P(T,S)$, plus a â€˜surprisalâ€™ term $ ln P(S)$ which - as we discussed above - just tells us how unexpected some sense data is. This looks like progress! . We can pull the $ ln P(S)$ out from under the integral because we have the requirement that: . âˆ«dTq(T)=1 int_{}^{}{ text{dT} q left( T right) = 1}âˆ«â€‹dTq(T)=1 . This is just the requirement that probabilities sum to one. Integration is linear, so we have: . DKL(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«dTÂ q(T)Â lnâ¡q(T)P(T,S)Â +âˆ«dTÂ q(T)Â lnâ¡P(S)D_{ text{KL}}(q(T)||P(T|S)) = int_{}^{}{ text{dT} q(T) ln{ frac{q left( T right)}{P left( T,S right)} }} + int_{}^{}{dT q(T) ln{P(S)}}DKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«â€‹dTÂ q(T)Â lnP(T,S)q(T)â€‹Â +âˆ«â€‹dTÂ q(T)Â lnP(S) . DKL(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«dT[q(T)lnâ¡q(T)P(T,S)Â ]+lnâ¡P(S)Ã—1D_{ text{KL}}(q(T)||P(T|S)) = int_{}^{} text{dT} left lbrack q left( T right) ln{ frac{q left( T right)}{P left( T,S right)} } right rbrack + ln{P left( S right) times 1}DKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))Â =âˆ«â€‹dT[q(T)lnP(T,S)q(T)â€‹Â ]+lnP(S)Ã—1 . Now, we define: . FÂ â‰¡âˆ«dT[q(T)lnâ¡q(T)P(T,S)Â ]F equiv int_{}^{} text{dT} left lbrack q left( T right) ln{ frac{q left( T right)}{P left( T,S right)} } right rbrackFÂ â‰¡âˆ«â€‹dT[q(T)lnP(T,S)q(T)â€‹Â ] . Giving us: . DKL(q(T)âˆ£âˆ£P(T|S))=F+lnâ¡P(S)D_{ text{KL}}(q(T)||P left( T middle| S right)) = F + ln{P(S})DKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))=F+lnP(S) . An important property of the KL Divergence is that it is always greater than or equal to zero. The formula above tells us that if our KL divergence went to zero (that is, if our R-density became a perfect approximation of our true posterior), we would have: . 0=F+lnâ¡P(S)0 = F + ln{P left( S right)}0=F+lnP(S) . F=âˆ’lnâ¡P(S)F = - ln{P(S)}F=âˆ’lnP(S) . If you havenâ€™t guessed already, the F we defined above is â€˜free energyâ€™. Now we have a key result in the Free Energy literature, one which Friston refers to all the time: . Free energy is an upper bound on surprisal! . We can see this because we know that the KL divergence is always greater than or equal to zero, implying: . DKL(q(T)âˆ£âˆ£P(T|S))â‰¥0D_{ text{KL}}(q(T)||P left( T middle| S right)) geq 0DKLâ€‹(q(T)âˆ£âˆ£P(Tâˆ£S))â‰¥0 . â‡’FÂ +Â lnâ¡P(S)â‰¥Â 0 Rightarrow F + ln{P left( S right)} geq 0â‡’FÂ +Â lnP(S)â‰¥Â 0 . FÂ â‰¥Â âˆ’lnâ¡P(S)F geq - ln{P(S)}FÂ â‰¥Â âˆ’lnP(S) . More specifically, free energy is the surprisal an organism experiences upon sampling some data, given a generative model. If youâ€™re up for it, you should try matching those words to the parts of the equations that encode them! . So far, we have found that this quantity â€˜free energyâ€™ is an upper bound on surprisal, and we notice that minimising it means we are approximating the true posterior P(Tâˆ£S)P(T|S)P(Tâˆ£S) . Another form of the free energy: . Weâ€™re going to need to massage this equation for F a bit more to see if something useful pops out. . Again, using the linearity of integration, and the properties of the logarithm, we have: . FÂ â‰¡âˆ«dT[q(T)lnâ¡q(T)P(T,S)Â ]F equiv int_{}^{} text{dT} left lbrack q left( T right) ln{ frac{q left( T right)}{P left( T,S right)} } right rbrackFÂ â‰¡âˆ«â€‹dT[q(T)lnP(T,S)q(T)â€‹Â ] . FÂ =Â âˆ«dTÂ q(T)Â lnâ¡q(T)Â Â âˆ’âˆ«dTÂ q(T)lnâ¡P(T,S)F = int_{}^{} text{dT}{ q(T) ln{q(T) } -} int_{}^{}{ text{dT} q(T) ln{P(T,S)}}FÂ =Â âˆ«â€‹dTÂ q(T)Â lnq(T)Â Â âˆ’âˆ«â€‹dTÂ q(T)lnP(T,S) . From statistical mechanics, we say that the expected value (or average) energy of a system is simply the sum over all energy states, times the probability of each energy state: . E[X]Â =Â âˆ‘iXiP(Xi)E lbrack X rbrack = sum_{i}^{}{X_{i}P(X_{i})}E[X]Â =Â iâˆ‘â€‹Xiâ€‹P(Xiâ€‹) . And in the continuous case: . Eâ¡[X]=âˆ«RxP(x)â€‰dx displaystyle operatorname {E} [X]= int _{ mathbb {R} }xP(x) ,dxE[X]=âˆ«Râ€‹xP(x)dx . We also have the entropy of a system (precisely the average surprise in the probability distribution) as: . H(X)=âˆ’âˆ‘i=1nP(xi)logâ¡P(xi) mathrm {H} (X)=- sum _{i=1}^{n}{ mathrm {P} (x_{i}) log mathrm {P} (x_{i}})H(X)=âˆ’i=1âˆ‘nâ€‹P(xiâ€‹)logP(xiâ€‹) . . Entropy can be a somewhat tricky term, but I think this way of thinking about it is fairly intuitive: itâ€™s just the amount you expect to be surprised by a given probability distribution. Some distributions are very tightly clustered around their average values, and so they are very unsurprising, hence low entropy. The opposite of this are the so-called maximum-entropy distributions, which means every sample is maximally surprising. . Equipped with these ideas, we define a function $E(T,S)$: . E(T,S)Â â‰¡âˆ’lnâ¡P(T,S) mathbf{Î•}(T,S) equiv - ln{P(T,S)}E(T,S)Â â‰¡âˆ’lnP(T,S) . And this allows us to rewrite our free energy term F as: . FÂ =Â âˆ«dTÂ q(T)Â lnâ¡q(T)Â Â âˆ’âˆ«dTÂ q(T)lnâ¡P(T,S)F = int_{}^{}{ text{dT} q(T) ln{q(T) } -} int_{}^{}{ text{dT} q(T) ln{P(T,S)}}FÂ =Â âˆ«â€‹dTÂ q(T)Â lnq(T)Â Â âˆ’âˆ«â€‹dTÂ q(T)lnP(T,S) . F=âˆ«dTÂ q(T)E(T,S)+âˆ«dTÂ q(T)lnâ¡q(T)F = int_{}^{}{ text{dT} q left( T right) mathbf{E} left( T,S right)} + int_{}^{}{ text{dT} q left( T right) ln{q left( T right)}}F=âˆ«â€‹dTÂ q(T)E(T,S)+âˆ«â€‹dTÂ q(T)lnq(T) . F=âˆ«dTÂ q(T)E(T,S)âˆ’(âˆ’âˆ«dTÂ q(T)lnâ¡q(T))F = int_{}^{}{ text{dT} q left( T right) mathbf{E} left( T,S right) } - left( - int_{}^{}{ text{dT} q left( T right) ln{q left( T right)} } right)F=âˆ«â€‹dTÂ q(T)E(T,S)âˆ’(âˆ’âˆ«â€‹dTÂ q(T)lnq(T)) . Which (check that you see this from above) looks like itâ€™s saying that â€˜free energyâ€™ is equal to an average energy, minus something that looks a little like a continuous version of the entropy10. This version of the formula is something youâ€™ll hear Friston refer to often, because itâ€™s analagous to the Helmholtz free energy from thermodynamics/statistical mechanics. The Helmholtz free energy is defined as the difference between the internal energy and the entropy of the system (multiplied by the temperature, but ignore that here). Here the term â€˜free energyâ€™ acquires some physical sense, being the quantity of energy in our system that is available to do useful work. . In the next post, weâ€™ll take the background we developed here and build on it. Weâ€™ll take a deeper look at the R and G densities and some simplifying assumptions that allow us to write neat versions of them. The result will show the deep connection between the Free Energy Principle and Predictive Coding in the brain. . If something here doesnâ€™t make sense, is clearly wrong, or got you interested, let me know here or on twitter @jnearestn If you donâ€™t want to wait for the next post, The Free Energy Principle for Action and Perception: A Mathematical Review is the best place to start out on your own, and I owe it a great deal in helping write this post! . Cover image: Andrestand on Flickr . The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in codeÂ &#8617; . | shoutout to the New York Times for going full 2020 and trying to ruin that tooÂ &#8617; . | See for example Fristonâ€™s reaction to a bunch of quants in this articleÂ &#8617; . | in fact, this multiplicity of disciplines seems to set off a percentage of peopleâ€™s BS-detectors, sort of like when Deepak Chopra starts invoking Quantum Mechanics as an explanation for everythingÂ &#8617; . | Fully realised enlightened Buddhists with no sense-of-self donâ€™t @ me yet please, itâ€™s just a starting pointÂ &#8617; . | if you donâ€™t like 37, just use whatever number you were told by the last person who pointed one of those COVID-screening-thermometer-guns at your headÂ &#8617; . | there is of course the question of â€˜why expect 37 degrees in the first place, why not 800 and then you can survive through more possible statesâ€™, to which the quick answer is â€œevolutionâ€ and evolutionary nichesâ€ â€“ something like: that temperature is the constraint that you, as a particular organism, are forced to satisfy because of the particular set of biochemical pathways you evolved to best fit into your environmental niche (which seems, at this point, to be Zoom Meetings, for some reason?)Â &#8617; . | 9 year old Jared knew a killer font when he saw oneÂ &#8617; . | watch this video by Aurelien Geron if you want a great intro to the KL-divergenceÂ &#8617; . | as Iâ€™m writing this, Iâ€™m learning that this continuous analogy of the entropy is not actually well-defined. Itâ€™s called differential entropy, and Claude Shannon apparently just wrote it down, assuming it was correct (okay, now I feel less bad for making the same assumption). It took E.T Jaynes to write down a better version called the â€˜Limiting Density of Discrete Pointsâ€™, which - at minimum - is a worse name than â€˜differential entropyâ€™. I donâ€™t know what effect the ill-definedness of continuous entropy has for the FEP, so thatâ€™s something to look into while I write part 2!Â &#8617; . |",
            "url": "https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html",
            "relUrl": "/2020/08/08/free-energy1.html",
            "date": " â€¢ Aug 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "3 Rules For Learning Difficult Things Online",
            "content": "This doesnâ€™t apply to people receiving a set online syllabus from a university or other institution. This is for people who, for whatever reason, didnâ€™t have the chance to take the math/physics/bio/compsi1 courses they wanted to at a university, and are now trying to learn something useful on their own. . 1. Advanced before Introductory . â€˜Introâ€™ courses are everywhere, and theyâ€™re largely useless2. Theyâ€™re fine if you want to keep up with conversation, but theyâ€™re too superficial to make your training useful. Coursera and other platforms like it are particularly full of these kinds of courses. . It is an unwritten rule of university professors that every course must begin with an introductory lecture which revises everything you would get from the â€˜beginnersâ€™ course, so youâ€™ll know if you are missing anything crucial before you begin. Another reason to go for the advanced course is because, bizarrely, introductory courses often end up leaving out exactly the â€˜advancedâ€™ arguments that make complex ideas make sense. Advanced courses will keep you interested for longer, because youâ€™ll have to really try to understand. If you can tolerate confusion, being forced to work it out will be more beneficial than comfortably following the boring intro course. . How? . If youâ€™re going to use Coursera, look for an advanced course. Even better, avoid Coursera and find the course a department actually uses to teach its students. Thereâ€™s an art to this, but if you can find the courseâ€™s webpage for students (start at the departmentsâ€™ webpage, go to a lecturerâ€™s bio and see if it lists any courses), there is often a private YouTube playlist lecture recordings, and PDFs of all lecture slides and readings. . Examples: . University of Toronto Machine Learning Courses Economics and Computing courses by Matt Weinberg . 2. Applications before Abstractions . If you want to learn something interesting, and difficult, it probably comes with a substantial list of prerequisites. Or, you have an intuition that learning [advanced concept X] might be useful in your work/research/Twitter-flame-wars. . This is probably Math. For several reasons, math3 is usually the bottleneck to learning the cool new thing you want to try. Your list of prerequisites might be something like: linear algebra, vector calculus, differential equations, abstract algebra, probability theory, differential geometry etc. . And you, being the kind of person who learns things online for fun, will go and track down all of the best complex analysis courses, and theyâ€™ll all be intended for math majors and youâ€™ll be perfectly miserable as the Prof writes down another list of assumptions about the function being holomorphic around p. If you love that kind of rigour, stick with it, but I suspect many people are put off because they just need the tools, for now. . How? . Go find courses which explicitly try to teach the ideas of that abstract discipline to non-experts. The benefit is that the Prof now canâ€™t assume sheâ€™s talking only to math majors, which may improve your experience. Bonus points if you learn something about the applied discipline too! . Examples: . Abstract Algebra for Theoretical Computer Scientists Applied Topology for Neuroscience Applied Category Theory . 3. Top-Down before Bottom-Up . There are two ways you can go about learning something difficult â€“ you can start at the bottom and build up: learning the prerequisites and the background details, slowly building up towards the big, useful, important, interesting ideas at the cutting edge of the field â€“ or you can start from the top and dig down: immersing yourself immediately in something big and exciting, which you donâ€™t fully understand yet, but which youâ€™re motivated to keep doing because youâ€™ve seen tangible results. Top-down also keeps you focused on essentials, because you only add detail as is necessary for your understanding. Already know how to multiply matrices, but donâ€™t remember how to find their eigenvalues? Then why sit through an entire linear algebra course? Youâ€™ll suddenly find yourself needing to learn about eigenvalues and you can spend an hour on YouTube getting up to speed (Tim Ferriss calls this â€œjust in time, not just in caseâ€). . The fast.ai Deep Learning course is the canonical example here. In the first lesson, you train a deep neural net to recognise several breeds of dogs and cats, at near-cutting-edge level. Over the course, you learn by tinkering with advanced models and learning to play around with the code. Itâ€™s probably the closest thing to Legitimate Peripheral Participation Iâ€™ve seen in an online course so far. . How? . Following rule 1 and 2 should have already got you a lot of the way here. Look for project-based courses, or see if you can just start with the coolest part of a discipline and pick up the prerequisites as you go. Donâ€™t fall into the trap of spending years â€œin preparationâ€, this is probably as bad as only ever doing â€œintroductoryâ€ courses. . Examples: . Fast.ai . I donâ€™t have any other good examples to hand, suggestions would be appreciated, and Iâ€™ll add them here! . I mostly have experience doing this for STEM domains. These might work for courses in the arts/humanities and so on, but thereâ€™s probably something important Iâ€™m missingÂ &#8617; . | I think that some introductory programming courses might be exceptions here â€“ or maybe languages generally?Â &#8617; . | I dream of a world where everyone has a browser plugin which just changes words to their favourite version. All the maths vs mathematics vs math people, all the American â€˜-izeâ€™ vs sensible â€˜-iseâ€™ people could just get on with it.Â &#8617; . |",
            "url": "https://jaredtumiel.github.io/blog/2020/05/12/three-rules.html",
            "relUrl": "/2020/05/12/three-rules.html",
            "date": " â€¢ May 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Your Algorithmic Reflection",
            "content": "Most people are aware that algorithms shape their online experience. Some people understand that the algorithmsâ€™ underlying incentive is to maximise time spent on-site and number of ad-clicks. Few people know how to use this to their benefit. . The 2016 US election saw the popularisation of the terms â€œfilter bubbleâ€ and â€œfake newsâ€. The first refers to algorithms selectively showing social media content that confirms your existing worldview and systematically excluding all but strawman versions of opposing viewpoints from your feed. The second is a consequence of this: the algorithms will propagate the most polarising, least-nuanced views because these naturally stir reaction and attract clicks. At some point, the view becomes so distorted itâ€™s patently false and yet wildly popular within a filter bubble. Iâ€™d flesh this out more, but CGP Grey has a wonderful short video explaining just this, so you should go watch that instead. . The Good, the Bad, and the Ugly of your Algorithmic Reflection . Combatting fake news and filter bubbles is vital, but here I want to ask what these algorithms teach us about ourselves and how we can use this knowledge to our advantage. . YouTube, Facebook, and the rest have spent a lot of money developing machine learning systems that will keep you hooked on their platform. They do this by accumulating information about what you seem to like, showing you some suggestions, learning from your reaction, and repeating the process 1. What each company is racing to do is build the most accurate model of you they can because an accurate model of you gives them the power to predict what you might like in the future. . This is my key argument: . Algorithmic content suggestion is here to stay | Algorithms try to understand your interests, personality, and identity to better serve you content you will enjoy - thus keeping you on-site longer | The way the algorithms do this is by observing your current browsing habits (what you do and donâ€™t click on, what you watch and for how long etc.) and the browsing habits of people â€˜like youâ€™ (people engaging with similar things and what else they seem interested in) and building a model of you based on this data 2 | In this way, our newsfeeds and suggestions become a reflection of us - our algorithmic reflection | Therefore, we should act in a way which puts the algorithms to work for us so that what is reflected back to us becomes both aspirational (because it represents us at our best) and self-fulfilling (in that consuming better content will shape us into better people, who thus desire better content). | Contrast this last point with The Unhappy Death-Spiral of social media that plays out as follows: . Open your platform of choice and scroll. The timeline is infinite, the videos on autoplay | Halt! The thumbnail has a cat and the all-caps caption ending in â€œğŸ”¥ğŸ”¥ğŸ”¥ğŸ˜‚ğŸ˜‚â€ has alerted you to the presence of something important. 41K heart reactions canâ€™t be wrong | Train the algorithms to predict that you value the mundane, the clickbaitey, the controversial-but-lacking-in-nuance | Receive suggestions which fulfill this prophecy, consume them in turn, reinforce the algorithmâ€™s model of you | Minutes pass and accumulate into hours. Bleary eyed and invariably feeling just a little worse about yourself, return to what you were meant to be doing. Bonus if you lament to a friend that you â€œjust donâ€™t have time for hiking or coffee or painting like you used toâ€ | . This is a pattern I have repeated more times than Iâ€™m comfortable admitting. Itâ€™s something I see my friends do, and itâ€™s something Iâ€™m sure most readers have experienced, especially if youâ€™re young enough not to have known a world without social media. . Weâ€™re collectively adrift in an ocean of online content and most of us are caught in the doldrums (the latitudes where sailors would spend months trapped, just waiting for a wind - an apt metaphor for the difficulty of escaping the Unhappy Death Spiral above). We find ourselves in these doldrums, directed there by algorithms tasked only with keeping us in the ocean. Itâ€™s here that we see our chance of escape, for the algorithms donâ€™t care where in the ocean you are, just that you remain 3. The doldrums are a popular destination for the algorithms to steer you into because they work on just about anyone. To see this effect in action, just look at the kind of content that YouTube shows a first-time user whoâ€™s just signed up:4 . A first look - I think the video game and car suggestions are the algorithmsâ€™ best guess for 20-something men? . Notice that just based off of my demographics, YouTubeâ€™s algorithm has identified some topics and videos they know from experience are likely to get me to stick around. Scrolling down a bit shows these suggestions. You can see my browsing location has made the algorithm select some local Soap Operas for me: . Not exactly Khan Academy, is it . I think the first thing to notice here is just how terrible most of these videos are. Many are clickbaitey; several fall into that strange category of simple things that humans find oddly satisfying, things which abuse some novelty switch deep inside us, like this video of someone testing to see if sharks can detect blood. . . I havenâ€™t watched it, so I donâ€™t know what happens. But 31 million people in the past week have. Thatâ€™s more people than live in all of Australia. Thatâ€™s so many people that you could have everyone in Australia watch it and share it with everyone in Puerto Rico and everyone in Slovenia and there would still be a million views unaccounted for. . My point here is not to denigrate content creators. If making this kind of YouTube video is what you do, then by all means continue. My point here is to say that for many people, this kind of content - and its analogues on Facebook, Instagram, and the like - is pushed by algorithms because it easily captures our attention without really improving anything else about us. And once weâ€™ve revealed a weakness for this kind of content, weâ€™re bound to be shown more like it, binding us to an algorithmic reflection of ourselves we donâ€™t really like. My point here is that we can do better! . Thereâ€™s an oft-repeated quote that you are the average of the five people youâ€™re closest to. Thereâ€™s another that says â€œyou are what you eatâ€. Weâ€™re treated by algorithms as the average of the five-million people closest to our demographic group and we are being force fed their media-diet. . Creating a better reflection - the case for algorithms . Consider for a second the power of these algorithms. They trawl the vast landscape of new content generated every day and manage to reliably identify the few pieces worth seeing. Of the 500 hours of YouTube videos uploaded every minute of every day, or the half a billion tweets sent daily, you see the tiny fraction that are worth it 5. At least in theory theyâ€™re worth watching - in practice theyâ€™re often just the most polarising, overstated, and clickbaitey. . Remember though, all the algorithms care about is that you use the site. If you could teach them that showing you content that makes you a reliably better person - in whatever way you want to define that - also keeps you using the site, you suddenly have the resources and technology of the worldâ€™s most powerful companies working to find more content that will do that. And as you do this more, you can refine your standards, so that the quality of content continues to rise, bringing you with it. . Suddenly, instead of a feed brimming with garbage, you have a legion of algorithms working tirelessly to pick the most nuanced, thoughtful, reasonable, and long-term-useful content from the morass and deliver it to you. Youâ€™ve gone from being collateral damage in the war on attention to purposefully using technology to improve yourself in lasting ways. . Before I go on and explain how to do this, I want to pause here because this is the deeper message of this essay: as new sources of information increase, we will be faced with the very real problem of sorting and prioritising it. For most of human history, weâ€™ve managed this ourselves, but in some areas weâ€™ve reached the limits of what we can reasonably sort 6. This puts us at a tipping point, where we need to increasingly offload the administrative burden of discovering, prioritising, and (increasingly) acting on 7 content to machine learning systems. If we do this without considering what those algorithms are optimising for, we risk handing a large part of our lives to systems not aligned with our terminal goals. Currently, they optimise for our attention and our ad-clicks, and this has given us fake news, political polarisation, and filter bubbles. Currently, we can still trick-or-teach them to show us content that both keeps our attention and fulfills some higher purpose of our own. This is only because time-on-site and consuming useful content are not mutually exclusive. However, the natural descent into clickbait that arises out of the competition for attention should remind us that optimising without thinking about second-order effects inevitably leads to unforeseen consequences. We want algorithms to decrease the time we spend on mindless admin. We need not be mindless in how we get there. . A guide to changing your algorithmic reflection . If you wanted to combat this, you might try some of the following (this guide mostly focuses on YouTube because itâ€™s particularly well suited to long-form content that could lead to real, lasting improvements, but similar actions on Facebook, Instagram, Twitter etc. will also work): . 1. Decide on a direction . Start by deciding on exactly what aspect of yourself you want to improve. This can be pretty general to start because the algorithms will naturally find and explore relevant material for you. All you have to do is make sure you only engage with content relevant to the part of you that you want to develop. . For me, I wanted to learn much more about machine learning and mathematics. I wanted to teach YouTube to find the most useful content about these topics. To keep this separate from my everyday YouTube, I created a new Google account and vowed to use it only for videos that would help me learn more about what I care about. To take away the hassle of remembering to switch between Google accounts, I set up a FireFox container (for Chrome users, this plug-in appears to do something similar, but I havenâ€™t used it). Firefox containers keep all your log-in information separate for different accounts on the same website, like this: . . Note how each instance of YouTube is giving me different recommendations, with the middle one giving the most relevant ones to machine learning. Now you can keep your different online personas separate! . 2. Reset the algorithms . If you donâ€™t want to create a new account, or you want to keep your current one, you can reset what the algorithms suggest for you by deleting some or all of your search and watch history. Simply head here, select â€˜Delete Activity byâ€™ and type in the keywords or time interval you want gone. . 3. Bonus - browser plugins that make the internet far better . DF YouTube (Firefox) optionally blocks sidebars, feeds, comments, suggestions at the end of videos | uBlock Origin (Firefox, Chrome) has long been my ad-blocker of choice. It far exceeds AdBlock | StayFocusd (Chrome) is great at blocking websites when you want to focus | Wikiwand is not well known but deserves to be. If you use Wikipedia as much as I do, youâ€™ll appreciate the look and feel this adds! | . Go Forth . Algorithms discover, surface, and propagate the content we see. This is unavoidable - desirable even. Everything you interact with on social media is teaching some system what you and people like you are interested in and value. The reflection of ourselves we currently see in the algorithms and on our feeds is distorted, muddied by what gets clicks. Remove the silt, let the waters settle, then move forward with purpose and direction! . Cover Photo by Alexandre DebiÃ¨ve on Unsplash . Am I really going to throw in 2 CGP Grey references in such close proximity? Yes. Watch this video for a basic overview of how algorithms and your feed play togetherÂ &#8617; . | For the Bayesians here: think of the people â€˜like meâ€™ (and my age and demographic info) as giving a prior probability for what Iâ€™ll like, and each piece of content I engage with as updates that refine the modelÂ &#8617; . | I view the process of removing yourself from this ocean as a separate (and also incredibly important) venture. For more, Cal Newportâ€™s new book, Digital Minimalism is a great startÂ &#8617; . | I made a dummy account using my real age and demographic, then logged in to YouTube to see what Iâ€™d be shownÂ &#8617; . | Seriously, try having a look at YouTube videos with less than 100 views, theyâ€™re generally terribleÂ &#8617; . | Do you know anyone who is happy or even content with the amount of email theyâ€™re currently receiving?Â &#8617; . | Think the automatic replies in Gmail or Google Calendar scheduling in your flights after you get your ticket confirmation emailÂ &#8617; . |",
            "url": "https://jaredtumiel.github.io/blog/2019/08/11/use-algorithms.html",
            "relUrl": "/2019/08/11/use-algorithms.html",
            "date": " â€¢ Aug 11, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, itâ€™s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://jaredtumiel.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://jaredtumiel.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}